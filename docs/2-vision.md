# Statement of vision for education in statistics and data science

<!-- particularly attractive: Candidates motivated to innovate around the teaching of both core statistical topics (e.g., estimation, hypothesis testing) and more nascent data science topics (e.g., visualization, data wrangling, transparent and reproducible workflows). -->

<!-- Asset: Experience with evidence-based teaching methods, curriculum development, course design, and other initiatives that advance the university’s ability to excel in its teaching and learning mandate.  -->

A major passion of mine is reframing Statistical Science from the lens of Data Science, by taking a "problem-first" approach. You can read about my vision in the first section, which is followed by a section describing how I go about executing this vision as educational leadership initiatives. 

## Statistics for Data Science

<!-- I imagine this section to be about how I think statistics  -->


<!-- This section (perhaps to be renamed or split into multiple) is my vision for Data Science, especially Statistics for Data Science. It's not what I'm doing for EL. -->

Why does Statistics need reframing for Data Science? Statistical Science tends to begin with assumptions / a modelling framework, then describing properties and methods that enable solutions to a problem. This is a "framework-first" approach. But Statistics for Data Science requires a "problem-first" approach, focussing on how to appropriately construct a modelling framework from real-world problems. These two approaches are not translatable, because real problems don't neatly satisfy assumptions nor come with a description of what output is appropriate for addressing the problem. A real problem requires critical evaluation as to what outputs and what assumptions are useful and realistic. This isn't assumption-_checking_, it's assumption-_building_.

By the way, even Applied Statistics might sometimes seem to take a problem-first approach, because it starts with a motivating problem. But the focus is still on describing properties of the newly defined framework, falling squarely within the field of Statistical Science. On the other hand, Statistics for Data Science must focus on how to deal with a wide range of problems, not just one problem type. This is a challenge that relies on the properties already described by Statistical Science, hence why both approaches are important!

My viewpoint on this has largely come about through the re-development of many courses and materials throughout the years. Formal redevelopments (or developments) included DSCI 531/532, DSCI 561/562, and BAIT 509; just about all other courses I've been involved with have had some level of redevelopment, such as STAT 545A/547M, DSCI 511, DSCI 574, and DSCI 551, amongst others.

The way I see it, teaching a problem-first approach for Statistics for Data Science still has a long way to go. A central and powerful approach that I've been adopting is to create content focussing on the _type of problem_ encountered in real-world problems, instead of the traditional Statistics topics. Perhaps the best example of this is DSCI 562 (Regression II), which I re-developed last year. Instead of teaching "Survival Analysis" and "Multiple Imputation", I teach "regression when data are censored" and "regression when data are missing". This changes the focus of the lessons to be about evaluating approaches for solving the type of problem, including evaluating the impact of ignoring the problem altogether (perhaps there's little to no impact!). This approach allows students to creatively come up with solutions to a variety of real world problems, which are sure to involve "combined" scenarios, such as a censored response with some missing data.

Sometimes, a traditional statistical topic is made up of more than one type of problem, resulting in a restricted way of thinking. For example, the topic of Generalized Linear Models (GLM's) can be split into two problems: regression when the range of the response is restricted, and using a distributional assumption to improve estimation performance. Both are valuable tools that expand beyond the scope of GLM's, but if just taught as GLM's, restricts students' thinking to the smaller range of problems that GLM's can handle. 
Determining the type of problem that a traditional statistical topic addresses is not always obvious. I found the GLM example to be tricky; here are some other tricky ones, from DSCI 551 (Probability for Data Science):

- "Probability Distributions" has the underlying problem of communicating an uncertain outcome.
- "Simulation" has the underlying problem of calculating probabilistic quantities (mean, probabilities, etc.) when the distributions or calculations are hard to come by.
- "Independence" is tricky because it's really "dependence" that is more interesting. Dependence, then, can have an underlying problem of garnering information from a dependent variable (or what I like to call "harvesting dependence").

This problem-first approach naturally promotes motivation in the classroom, because the topics are designed to be practical from the get-go. Sample real-world problems become important, and where possible, I draw from my capstone and industry experience to create compelling examples. Perhaps my favourite is the ["ships making demands at the port" example in DSCI 551](https://ubc-mds.github.io/DSCI_551_stat-prob-dsci/lectures/simulation.html#multi-step-simulations-10-min), which I encountered in one of last year's capstone projects, to demonstrate simulation under a more complicated situation. Another favourite is flood forecasting; another, determining the effect of blue-light-blocking glasses on sleep.

An interesting thing that happens when taking a problem-first approach is that it brings to light areas of Statistics that are useful, yet are not receiving much attention. Two examples are quantile regression or distributional forecasting instead of mean regression. Another example is extreme value modelling (EVM), which becomes invaluable for problems that have heavy tailed data, such as at least one capstone project from Seahorse Strategies last year (the team should have used EVM, but did not, because we didn't teach it in MDS), as well as organizations tackling the issue of flood forecasting and flood frequency assessments. Interestingly, these topics tend to be deemed difficult, probably because of our community's obsession with the mean, as well as a general misunderstanding of the role of distributions outside of just being a "necessary statistical assumption". I believe these topics are more accessible than they're made out to be, if taught from a problem-first lens, because the mathematical details are inconsequential.

## Educational Leadership

<!-- EL = an activity taken at UBC and elsewhere to advance innovation in teaching and learning with impact beyond one’s classroom -->


<!-- Now that I've described my vision for data science, it's now time to describe how I've been (and intend to) make this EL. Provide evidence of goodness where possible. -->

<!-- from Mike: what "educational leadership" (EL) means to you and what EL you might pursue in the future. Vision for long-term career growth towards Prof of Teaching, even if it's tentative. -->

<!-- must provide evidence for promise of educational leadership at UBC and nationally or internationally -->

I'm finding that there's a general lack of problem-first education in Statistics, as well as a lack of good examples, but this presents opportunity. I envision being a leader in building Statistics for Data Science -- it's actually a major component of my career and my life. This section describes my strategy plan for having an impact both within and outside of the classroom walls, including some of the things that I've accomplished already.

Perhaps the simplest approach I've taken is making my course material public, which I intend to continue doing. People are starting to notice. See the figure for activity from my most recent tweet of my most recent release, for DSCI 551. Also, the [STAT 545A/547M content](https://stat545guidebook.netlify.com/) is still receiving a lot of global attention, largely thanks to Dr. Jenny Bryan's original efforts with the course (although the content needs more upgrading than I've had capacity to supply).

![DSCI 551 tweet activity](./img/551_tweet.png)

I like to think beyond the structure of a classroom, by developing material intended to be used by a wider range of people than those who refer to course notes. So far, this means writing a book focussing on regression analysis from a problem-first perspective. It's far from done, but it's also far from just starting out. I'm calling it "[Interpreting Regression](https://interpreting-regression.netlify.com/)". For maximum return on investment, I try to write course notes that can be easily adapted to the wider audience of the book. I believe distributional forecasting deserves its own book as well, and would include approaches like extreme value modelling, and techniques like copula modelling. These topics are more practical and accessible than people think.

I also see the need for new R packages for making Statistics more accessible for Data Science. I made some S3 Object Oriented R packages during my PhD with many useful functions, but being my first R packages, they have many deficiencies, including a lack of vignettes, too many overly complicated functions, and a lack of tests. I have ideas for packages to replace these older ones, as well as altogether new ones. My goal for each one is to submit each to CRAN and ROpenSci, publishing a corresponding paper to the Journal of Statistical Software, as well as making helpful vignettes. Here are my ideas:

- The [`distplyr`](https://github.com/vincenzocoia/distplyr/) package (which is already underway) will make it easier to work with univariate distributions, including empirical ones, for the promotion of distribution-based analyses. This makes a distribution an S3 object, which can then be manipulated. This would, for example, aid in the teaching of mixture distributions, and aid in the creation of extreme value models using functions like `distplyr::right_graft()` or `distplyr::left_graft()`.
- The [`coperate`](https://github.com/vincenzocoia/coperate/) package is similar to `distplyr` in that it makes it easier to work with copulas by making a copula an S3 object. This will replace the [`copsupp`](https://github.com/vincenzocoia/copsupp/) package, one of my deficient R packages from my PhD days, yet still has lots of useful functionality.
- Modelling packages to go alongside the above two.

In short, I'm aiming to lead the development of a suite of tools that have a "wow" quality, so that they become go-to resources for people needing help for Statistics for Data Science to satisfy their unique needs. Aside from books, public course content, and R packages that I've mentioned above, I'm curious as to what other medium would appeal to other aspects of learning. R shiny web apps is one such medium that is on my radar. For example, the impact of multicollinearity is much easier to grasp by visualizing a regression plane slicing a data cloud, allowing the user to change the correlation between the covariates. Other media are only in the space of my imagination, but that's where things begin! Maybe one day I will in fact start an engaging podcast with my colleagues, or curate a monthly "Statistics for Data Science Box" containing books or other resources for busy professionals wanting to grow and learn. The possibilities are endless.

A completely different area where I would like to provide leadership is in teaching educators how to go about teaching Statistics for Data Science, because the field is so new. I of course don't have all the answers, so perhaps the best medium for this is to host workshops on Data Science education. In fact, I've already helped kick off a vision for an international workshop by spearheading the creation of a vision for MDS. Related, I believe it's important to be stewards of Data Science by promoting responsible use, which is reflected in our material, but also in this workshop / conference.

Because a problem-first approach is so critical for Statistics for Data Science, I believe a final area of leadership is to exercise my data science skills, or as Stephen Covey says in his book "The 7 Habits of Highly Effective People", to "sharpen the saw". Just as it's important for us to take time to stay healthy, it's also important as an educator to keep my Data Science skills sharpened, as well as up to pace with a changing field. Whether or not this is considered EL, it's important so that I can give students the most relevant and best education I can possibly give. Simple ways of remaining sharp, or at least up to date, is to subscribe to relevant newsletters such as RStudio's and ROpenSci's, and stay involves with the Data Science community on Twitter. But I believe it's important to actually do a bit of work for external organizations to help them with their data science needs. I did this last summer with BGC Engineering by dedicating a small amount of time each week, and this opened my eyes to their values and their data science problems, which I can then bring back to the classroom.
