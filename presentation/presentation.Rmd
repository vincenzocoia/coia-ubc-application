---
title: "presentation"
output: ioslides_presentation
---

```{r}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE, fig.width = 3, fig.height = 2, 
					  fig.align = "center", warning = FALSE)
```


# Part I: Teaching Strategy (<= 30 min)

Some strategies for teaching the requested topic, __sampling distribution of the sample mean__.

## Prerequisite Knowledge

> Clarify the prerequisite knowledge that you expect the students to know before they engage with the topic.

- Sampling from a population (distribution).
- How to calculate the sample mean.
- The concept of a sample mean vs. the true mean.
- LLN. We've so far put aside the question of "how good" the sample mean is when estimating the true mean.

## Learning Outcomes

> Describe some learning outcomes you would want your students
to be able to attain on the topic by the end of the course.

From this lecture, students are expected to be able to:

- Describe the difference between a sampling distribution and a population distribution.
- Use a sampling distribution to describe how well the sample mean estimates the true mean.

**Hint**: Use these learning objectives when studying for your exam. 

## Difficulties

> Difficulties students might have learning the topic. What difficulties would you
expect students to have when they engage with this topic?

- Sample size vs. number of iterations
- Sampling distribution vs. population distribution.

## Student Engagement and Learning

> Describe how you would engage the students with
the topic. How would you assist them in attaining your learning outcomes? Your
discussion can involve activities both in and out of scheduled class time.

- __In pairs, use R to generate their own sample, and calculate the sample mean, to come up with a sampling distribution.__
- Think-pair-share
- Books: SLR, modern dive
- Office hours
- Lab and lab assignment

## Technology

> Technology. What, if any, learning technologies can assist in the learning of the
topic?

- Some platform for everyone to submit their means.
- R, for them to generate and calculate (build on skills from yesterday)

## Assessment

> Assessment. How would you assess students' attainment of your learning outcomes
for this topic? Describe examples of both formative and summative assessment you
may use.

- Think-pair-share as an informal way of probing.
- Office hours
- Quiz questions:
	1. Consider the two distributions below. One of them is a population distribution, and the other is a sampling distribution of a mean of an iid sample. Which one is the sampling distribution?
	2. Why is it impossible for the sampling distribution of the mean (of an iid sample) to have a larger variance than the population distribution? Answer using no more than two **brief** sentences.

Answers:
	
1. B
2. Could say:
	- Because the variance of a mean is $\sigma^2 / n$, which cannot be larger than the population variance $\sigma^2$


```{r}
ggplot(tibble(x = -3:3), aes(x)) +
	stat_function(fun = dnorm, aes(linetype = "A")) +
	stat_function(fun = dnorm, args = list(sd = 1 / sqrt(10)), aes(linetype = "B")) +
	labs(y = "Density") +
	theme_bw() +
	scale_linetype_discrete("")
```


- Lab assignment questions:
	- ...

# Part II: Teaching Practice (<= 30 min)

## Engagement and Prerequisites 

The bolded one.

Remember:

- Students learned how to generate a sample in R from last class.
- Students learned how to compute the mean of a numeric vector in R from last class.
- Students know how to conceptually build a histogram, and know how to read one, but not necessarily how to make one. I'll provide the code for that. 

## Class

### Orientation

#### Review:

- So far, we've learned about *population distributions*, and how they can be used to explain the uncertainty of an unknown outcome. Here are the population distributions of three examples we've been working with:

```{r}
library(tidyverse)
dir("supplementary", full.names = TRUE) %>% 
	walk(source)
cowplot::plot_grid(
  tibble(x = expense$qdist(c(0, 0.99))) %>%
    ggplot(aes(x)) +
    stat_function(fun = expense$ddist) +
    theme_bw() +
    # ylab("Density") +
    scale_x_continuous("Monthly Expense", labels = scales::dollar_format()) +
  	scale_y_continuous("Density"),
  tibble(x = c(-0.5, 1.5)) %>% 
    ggplot(aes(x)) +
    stat_function(fun = octane$ddist) +
    theme_bw() +
    labs(x = "Octane Purity",
         y = "Density"),
  ggplot(los$pmf, aes(ndays, p)) + 
  	geom_col() +
  	ylab("Probability") +
  	xlab("Length of Stay") +
  	theme_bw(),
  nrow = 1
)
```
- We saw that there are two "versions" of the mean:
	- The __empirical mean__ is calculated using data, and is an average: $1/n \sum_i X_i$
	- The __true mean__ is what the empirical mean converges to as we collect more and more data. It can be calculated from a population distribution's density as $\int_x x f(x) dx.$
	- Hence, the empirical mean is an _estimate_ of the true mean.

#### Today's topic

If the empirical mean is an _estimate_ of the true mean, how can we communicate __how good__ of an estimate it is? The answer is, through a type of distribution called a __sampling distribution__. 



#### Concept 1

**A combination of random variables is also a random variable.**

For example, if $X$, $Y$, and $Z$ are random variables, then...

- $X^Y$ is also a random variable.
- $\exp(X) + Z\sin(Y)$ is also a random variable.
- ...

Remember, even though these quantities are made up of many random variables, they still calculate to a single number -- hence why they amount to a new single random variable.

#### Concept 2

**The distribution of an empirical mean is called a _sampling distribution_**.

For example, consider the mean of 10 observations of **Length of Stay** (say, the first 10 days of March). If we call those observations $X_1, X_2, \ldots, X_10$, then their mean
$$\bar{X} = \frac{X_1 + X_2 + \cdots + X_{10}}{10}$$ 
is a random variable (because each of $X_1, X_2, \ldots, X_10$ are), and this random variable's distribution is called a _sampling distribution_. 

#### Demonstration

Let's use simulation to view the sampling distribution of 10 **Length of Stay** observations.

Steps:

1. Everyone generate their own 10 observations from the **Length of Stay** distribution, and calculate the mean.
	- To start you off, here is code to generate 10 observations:

```
sample(1:5, size = 10, replace = TRUE, prob = c(0.25, 0.35, 0.2, 0.1, 0.1))
```

2. Submit your mean to the above google form.


#### Other Examples

Here are the sampling distributions of the mean of 10 observations for the three examples we're considering. They are obtained by obtaining 1000 means each.

```{r}
# N = number of iterations, n = sample size, rdist = function that
# generates a sample.
set.seed(1)
get_means <- function(N, n, rdist) {
	map_dbl(1:N, function(x) mean(rdist(n)))
}
tribble(
	~ example, ~ rdist,
	"Expense", expense$rdist,
	"Octane", octane$rdist,
	"Length of Stay", los$rdist
) %>% 
	transmute(example = as_factor(example),
			  means = map(rdist, ~ get_means(1000, n = 10, rdist = .x))) %>% 
	unnest(means) %>% 
	ggplot(aes(means)) +
	facet_wrap(~ example, scales = "free") +
	geom_histogram(aes(y = ..density..), bins = 25) +
	labs(x = "Empirical Mean",
		 y = "Density") +
	theme_bw()
```




# Part III: Vision (<= 20 min)

## The Question

I'm asked to respond to the following:

> How do the current changes in Data Science and/or Statistics education inform your
vision of high-quality education and degree programs (minor and/or major) in these
areas?

## What "changes"?

The major change is the birthing of a new subject, _Data Science_, which I believe is different from Statistics. In fact, it's not even a subset, and is combined with computer science. 

- Statistics leans towards __describing__ a framework of assumptions, sometimes motivated from a real problem (Applied Statistics).
- Data Science leans towards __building__ a framework of assumptions appropriate for addressing a real problem.

Both are important.

Problem -> Assumption Framework -> Properties

## Example: GLM for Statistics

The logical sequence for Statistics is:

- Introduce real data (maybe), and an overall objective (maybe).
- Start with the framework Y|X ~ F_Theta(x), g(E(Y|X)) = eta_x, for some F in an Exponential family, where Theta are its parameters, and g is a link function.
- Put lots of focus on explaining things that stem from the framework: ways of fitting Theta, how to obtain predictions, how to evaluate model fit, etc.

Important for ensuring GLM's are available as a tool.

## Example: GLM for Data Science

How does this inform the way I teach? I focus on how to build a framework of assumptions that is most appropriate for the problem at hand. This means presenting a problem, generalizing it, and addressing the pros and cons of different approaches. 

For example, instead of teaching survival analysis, I teach "regression when the response is censored". Or, instead of GLM's, "regression when the range of the response is restricted".

Specifically, let's look at GLM's. 
This is important! We need statisticians to describe the in's and out's of different frameworks so that we can use these frameworks. It can involve a lot of complicated mathematics and simulations. But it leaves the practitioner questioning why we "need" a link function, why we "need" these specific link functions, and why we "need" the distributional assumption.

The assumptions are usually stated right away, treated as requirements, sometimes receiving attention as to how to assess their validity. But a data scientist needs to recognize that the word "assumption" is not appropriate, but rather "approximation", because assumptions are almost never true. And further to that, recognize the value of making this approximation over not making the approximation at all, as well as why we're making _this specific_ assumption over the infinitely many other things we could have considered. 

The logical sequence for data science might look like: 

- Introduce real data and an overall objective.
- Describe why a simple approach _might_ fall short, and how sometimes this might not be a problem.
- Open the space for thinking about alternative possibilities.
- Indicate the decisive framework, and how to work with it.

Example:

- Example: binary response, want to learn the relationship between X and Y.
- Problem with simple approach: Linear regression overflowing boundaries, though only sometimes.
- Open the space for alternatives: Instead of looking for how a change in X _linearly_ increases the _probability_ of success in Y, what could we change to gain information on how X influences Y?
- Indicate the framework, and how to work with it:
	- Framework: g(p(x)) = beta0 + beta1 x
	- New interpretation of beta1. 
	- Look at what other link functions allow for interpretations of beta1.
	- Indicate that LS does a poor job with estimation, so add a distributional assumption together with MLE to improve estimation (another topic on its own!)
	- Teach how to fit this using code (R and/or python).

__I believe this approach is novel__, as it requires understanding how a technique truly fits into the world -- and as they say, this requires being able to explain the technique to your grandmother. It requires diving into the tricky question of why we set up the framework the way it is, and not another way. The trouble is -- and this is where the educational leadership comes from -- many statistical techniques aren't designed to be described to your grandmother. For example, I haven't seen anyone treat link functions and the distributional assumption differently.

__I believe this approach almost always requires very little math and technical skills__, and is therefore accessible to a wider audience than in the statistics-focussed sequence. Topics like quantile regression and extreme value modelling become undergraduate topics instead of advanced graduate topics. Also, because there's less of a deeper dive into details, it's possible to cover more ground in a smaller amount of time (as is evidenced by MDS).

__I believe statistics is not being replaced by machine learning__, because not everything is a prediction problem. 

When students learn the logic behind setting up various statistical frameworks, they are able to generalize and build their own unique framework for solving a real problem. 

Vision of the dsci streams at UBC:

- MDS: training expert professionals in data science.
- STAT 545: training academics to manage their analysis in a sane way using EDA. Not model-focussed.
- Minor: be a proficient member of a data science team whose primary focus is more on the subject matter of the problem, yet is able to hold intelligent conversations with data scientists, make informed decisions as to how to tackle a problem at a high level, and perform a basic analysis on their own if they need to do something on their own. 
